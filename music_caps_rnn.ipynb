{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce5a995-f8f5-4588-8b44-d88aab75a028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.1.4)\n",
      "Collecting torchvggish\n",
      "  Using cached torchvggish-0.2-py3-none-any.whl\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Collecting soundfile\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.4)\n",
      "Collecting torch==2.4.1 (from torchaudio)\n",
      "  Using cached torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.4.1->torchaudio) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.1->torchaudio) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.4.1->torchaudio) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.4.1->torchaudio) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.1->torchaudio) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.4.1->torchaudio) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1->torchaudio)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1->torchaudio)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1->torchaudio)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1->torchaudio)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1->torchaudio)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1->torchaudio)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1->torchaudio)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1->torchaudio)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1->torchaudio)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1->torchaudio)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1->torchaudio)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.1->torchaudio)\n",
      "  Using cached triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->torchaudio)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Collecting resampy (from torchvggish)\n",
      "  Using cached resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.5.10)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numba>=0.53 in /opt/conda/lib/python3.10/site-packages (from resampy->torchvggish) (0.59.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.53->resampy->torchvggish) (0.42.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.4.1->torchaudio) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.4.1->torchaudio) (1.3.0)\n",
      "Using cached torchaudio-2.4.1-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
      "Using cached torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
      "Using cached resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, soundfile, resampy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvggish, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0.post304\n",
      "    Uninstalling torch-2.0.0.post304:\n",
      "      Successfully uninstalled torch-2.0.0.post304\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.4.1 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.4.1 which is incompatible.\n",
      "fastai 2.7.15 requires torch<2.4,>=1.10, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 resampy-0.4.3 soundfile-0.12.1 torch-2.4.1 torchaudio-2.4.1 torchvggish-0.2 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchaudio pandas torchvggish nltk soundfile numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca30eead-77de-4210-a32b-ac33a1ffaec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import gc\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# 1. Import libraries and set up environment\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] >= self.freq_threshold:\n",
    "                    if word not in self.stoi:\n",
    "                        self.stoi[word] = idx\n",
    "                        self.itos[idx] = word\n",
    "                        idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]\n",
    "# 3. Dataset and DataLoader\n",
    "\n",
    "class MusicCapsDataset(Dataset):\n",
    "    def __init__(self, csv_files, audio_dir, vocab, fixed_length=160000, n_mels=128, target_sample_rate=16000):\n",
    "        # Merge all CSVs into a single DataFrame\n",
    "        dataframes = [pd.read_csv(csv_file) for csv_file in csv_files]\n",
    "        self.data = pd.concat(dataframes, ignore_index=True)\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        print(self.data.shape)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.vocab = vocab\n",
    "        self.fixed_length = fixed_length\n",
    "        self.n_mels = n_mels\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.target_sample_rate,\n",
    "            n_mels=self.n_mels\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_id = self.data.iloc[idx]['ytid']\n",
    "        audio_path = os.path.join(self.audio_dir, f\"{audio_id}.wav\")\n",
    "        caption = self.data.iloc[idx]['caption']\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sample_rate)(waveform)\n",
    "\n",
    "        if waveform.size(1) > self.fixed_length:\n",
    "            waveform = waveform[:, :self.fixed_length]\n",
    "        else:\n",
    "            pad_length = self.fixed_length - waveform.size(1)\n",
    "            waveform = F.pad(waveform, (0, pad_length))\n",
    "\n",
    "        mel_spectrogram = self.mel_spectrogram(waveform)\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]] + self.vocab.numericalize(caption) + [self.vocab.stoi[\"<EOS>\"]]\n",
    "\n",
    "        sample = {\n",
    "            'audio': mel_spectrogram.squeeze(0).T,  # Transpose to match (time, n_mels)\n",
    "            'caption': torch.tensor(numericalized_caption),\n",
    "            'original_caption': caption,\n",
    "            'path': audio_path,\n",
    "            'sample_rate': self.target_sample_rate\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "def pad_sequence(batch):\n",
    "    batch = [item['caption'] for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    return batch\n",
    "\n",
    "def collate_fn(data):\n",
    "    audio = [item['audio'] for item in data]\n",
    "    captions = [item['caption'] for item in data]\n",
    "\n",
    "    # Pad audio sequences to the same length\n",
    "    audio_lengths = [len(a) for a in audio]\n",
    "    max_audio_length = max(audio_lengths)\n",
    "    padded_audio = torch.zeros(len(audio), max_audio_length, audio[0].size(1))\n",
    "    for i, a in enumerate(audio):\n",
    "        padded_audio[i, :len(a), :] = a\n",
    "\n",
    "    # Pad caption sequences to the same length\n",
    "    caption_lengths = [len(c) for c in captions]\n",
    "    max_caption_length = max(caption_lengths)\n",
    "    padded_captions = torch.zeros(len(captions), max_caption_length).long()\n",
    "    for i, c in enumerate(captions):\n",
    "        padded_captions[i, :len(c)] = c\n",
    "\n",
    "    original_captions = [item['original_caption'] for item in data]\n",
    "    paths = [item['path'] for item in data]\n",
    "\n",
    "    return {'audio': padded_audio, 'caption': padded_captions, 'original_caption': original_captions, 'path': paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58151307-56f8-441a-b1f4-6bcdb5b19351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.gru = nn.GRU(input_size=256*input_dim//16, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x.unsqueeze(1)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        b, c, t, f = x.size()\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(b, t, c * f)\n",
    "        output, hidden = self.gru(x)\n",
    "        return output, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.embedding.num_embeddings\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[:, t, :] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if random.random() < teacher_forcing_ratio else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc7faac-22c7-4136-b266-d56ce1f08aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            audio = batch['audio'].to(device)\n",
    "            captions = batch['caption'].to(device)\n",
    "\n",
    "            output = model(audio, captions, 0)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].contiguous().view(-1, output_dim)\n",
    "            captions = captions[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, captions)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    early_stop_limit = 3\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch_idx, batch in progress_bar:\n",
    "            audio = batch['audio'].to(device)\n",
    "            captions = batch['caption'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(audio, captions)\n",
    "\n",
    "            output = output[:, 1:].reshape(-1, output.shape[2])\n",
    "            captions = captions[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, captions)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "        avg_val_loss = evaluate_model(model, val_dataloader, criterion, device)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            early_stop_count = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        # else:\n",
    "        #     early_stop_count += 1\n",
    "        #     if early_stop_count >= early_stop_limit:\n",
    "        #         print(\"Early stopping triggered\")\n",
    "        #         break\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c5f087-adc9-4917-abdf-d434f192e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, audio_path, vocab, device, fixed_length=160000, n_mels=128, target_sample_rate=16000, max_length=50):\n",
    "    # Preprocessing steps\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=target_sample_rate,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    \n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)(waveform)\n",
    "\n",
    "    if waveform.size(1) > fixed_length:\n",
    "        waveform = waveform[:, :fixed_length]\n",
    "    else:\n",
    "        pad_length = fixed_length - waveform.size(1)\n",
    "        waveform = F.pad(waveform, (0, pad_length))\n",
    "\n",
    "    mel_spectrogram = mel_spectrogram(waveform)\n",
    "    mel_spectrogram = mel_spectrogram.squeeze(0).T.unsqueeze(0).to(device)  # Transpose to match (time, n_mels) and add batch dimension\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(mel_spectrogram)\n",
    "        input_token = torch.tensor([vocab.stoi[\"<SOS>\"]]).to(device)\n",
    "        caption = []\n",
    "        print(f\"{mel_spectrogram.shape=}\")\n",
    "        print(f\"{encoder_outputs.shape=}\")\n",
    "        print(f\"{hidden.shape=}\")\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            output, hidden = model.decoder(input_token, hidden, encoder_outputs)\n",
    "            top1 = output.argmax(1)\n",
    "            if top1.item() == vocab.stoi[\"<EOS>\"]:\n",
    "                break\n",
    "            caption.append(top1.item())\n",
    "            input_token = top1\n",
    "\n",
    "    caption_text = ' '.join([vocab.itos[token] for token in caption])\n",
    "    return caption_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f38b7dc4-e30d-41cf-8963-e8942880c3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6312, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Load and prepare data\n",
    "# csv_files = ['musiccaps-public.csv']\n",
    "csv_files = ['mayo_final_final.csv', 'musiccaps-public.csv']\n",
    "audio_dir = './music_data/music_data'\n",
    "freq_threshold = 5\n",
    "\n",
    "dataframes = [pd.read_csv(csv_file) for csv_file in csv_files]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "# Split dataset\n",
    "dataset = MusicCapsDataset(csv_files=csv_files, audio_dir=audio_dir, vocab=None)  # Pass vocab=None for now\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = math.trunc((len(dataset) - train_size) / 2)\n",
    "random_seed = 42\n",
    "generator = torch.Generator().manual_seed(random_seed)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size + 1, val_size], generator)\n",
    "\n",
    "# Build vocabulary from training set\n",
    "train_captions = [df.iloc[idx]['caption'] for idx in train_dataset.indices]\n",
    "vocab = Vocabulary(freq_threshold)\n",
    "vocab.build_vocabulary(train_captions)\n",
    "\n",
    "# Update datasets with vocab\n",
    "train_dataset.dataset.vocab = vocab\n",
    "val_dataset.dataset.vocab = vocab\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Model parameters\n",
    "INPUT_DIM = 128\n",
    "OUTPUT_DIM = len(vocab)\n",
    "ENC_HIDDEN_DIM = 512\n",
    "DEC_HIDDEN_DIM = 512\n",
    "ENC_LAYERS = 2\n",
    "DEC_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "EMB_DIM = 256\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, ENC_HIDDEN_DIM, ENC_LAYERS, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, DEC_HIDDEN_DIM, DEC_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "#Initialize model weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# scheduler = None\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1f1011d-37b2-43c0-8774-2864c08a0a9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|██████████| 316/316 [03:04<00:00,  1.72it/s, loss=3.98]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Train Loss: 4.5816, Val Loss: 6.3014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6: 100%|██████████| 316/316 [02:57<00:00,  1.78it/s, loss=4]   \n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], Train Loss: 3.7440, Val Loss: 6.3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6: 100%|██████████| 316/316 [02:51<00:00,  1.85it/s, loss=3.52]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], Train Loss: 3.5106, Val Loss: 6.5204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|██████████| 316/316 [02:51<00:00,  1.84it/s, loss=3.39]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6], Train Loss: 3.3840, Val Loss: 6.6288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6: 100%|██████████| 316/316 [02:49<00:00,  1.86it/s, loss=3.5] \n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6], Train Loss: 3.2575, Val Loss: 6.8877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6: 100%|██████████| 316/316 [02:47<00:00,  1.88it/s, loss=3.31]\n",
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], Train Loss: 3.1671, Val Loss: 6.7900\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=6, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6b03c8-d73d-4645-8002-df3e62388acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'rnn_exp3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73b7cc4b-634a-4760-a9ea-6dc6e5e09ad7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10,) and (6,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# train_loss = [4.4672, 3.6907, 3.4250, 3.3041, 3.2101, 3.1043, 3.0370, 3.0107, 2.9225, 2.8537]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# val_loss = [6.2467, 6.3386, 6.6667, 6.3351, 6.7259, 6.4852, 6.5739, 7.1104, 6.8335, 7.0032]\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mvisualize_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m, in \u001b[0;36mvisualize_loss\u001b[0;34m(eval_losses, train_losses, number_of_epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinestyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, number_of_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), train_losses, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss Over Epochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/pyplot.py:3590\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3594\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3595\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1724\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1724\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1726\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axes/_base.py:499\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    496\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (6,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_loss(eval_losses, train_losses, number_of_epochs):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, number_of_epochs + 1), eval_losses, marker='o', linestyle='-', color='b', label='val')\n",
    "    plt.plot(range(1, number_of_epochs + 1), train_losses, marker='o', linestyle='-', color='r', label='train')\n",
    "    plt.title('Validation Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper left', fontsize='large', frameon=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# train_loss = [4.4672, 3.6907, 3.4250, 3.3041, 3.2101, 3.1043, 3.0370, 3.0107, 2.9225, 2.8537]\n",
    "# val_loss = [6.2467, 6.3386, 6.6667, 6.3351, 6.7259, 6.4852, 6.5739, 7.1104, 6.8335, 7.0032]\n",
    "\n",
    "visualize_loss(val_losses, train_losses, number_of_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695339f8-1e77-4f3c-b799-9663a855478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./best_model_poc.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629775e-ee36-4fb3-a965-ffa086555011",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(audio_path='../lixlj-khjq2.wav', vocab=vocab, model=model,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3f1bfe-c703-439c-a1de-d9bf32c1901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "INPUT_DIM = 128\n",
    "OUTPUT_DIM = len(vocab)\n",
    "ENC_HIDDEN_DIM = 512\n",
    "DEC_HIDDEN_DIM = 512\n",
    "ENC_LAYERS = 2\n",
    "DEC_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "EMB_DIM = 256\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, ENC_HIDDEN_DIM, ENC_LAYERS, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, DEC_HIDDEN_DIM, DEC_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e275c03-1ffe-4fff-98d9-c9fb8f65994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./best_model_poc.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f340c-3cd7-46fa-9c76-f10ce5c3babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = infer(audio_path='../lixlj-khjq2.wav', vocab=vocab, model=model,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf1383-2614-4b48-bac2-f853f159f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac63ade-0061-461c-a3b9-97dfebd93db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the model name\n",
    "llm_model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name, use_auth_token=\"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\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(llm_model_name, use_auth_token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3028a-8da5-46bd-b865-cbe5229f709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input text (instruction)\n",
    "input_text = f\"I'll provide you with a song description, I need you to generate an image description that reflects the same feelings and messages of the song. The image description will be fed into a text to image model in order to generate images. please generate only an imagae description, and it needs to be a maximum of 77 tokens. description - {description}\"\n",
    "inputs = llm_tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = llm.generate(**inputs, max_length=400)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c565d7-a591-4514-9e05-d94ddf395e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load the model\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "pipe = pipe.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798f93d-de55-4a0e-a60a-253abbc8d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "\n",
    "# Generate an image from a prompt\n",
    "image = pipe(generated_text.split('\\n')[-1]).images[\n",
    "# Save the image\n",
    "image.save(\"generated_image.png\")\n",
    "display.display(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
