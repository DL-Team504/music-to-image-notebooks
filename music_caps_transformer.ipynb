{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7d679f8-793f-418a-9d7f-d278af3ca19e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.1.4)\n",
      "Collecting torchvggish\n",
      "  Using cached torchvggish-0.2-py3-none-any.whl\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Collecting soundfile\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.4)\n",
      "Collecting torch==2.4.0 (from torchaudio)\n",
      "  Using cached torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchaudio) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchaudio) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchaudio) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchaudio) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchaudio) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchaudio) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0->torchaudio)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0->torchaudio)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0->torchaudio)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0->torchaudio)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0->torchaudio)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0->torchaudio)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0->torchaudio)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0->torchaudio)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0->torchaudio)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->torchaudio)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0->torchaudio)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0->torchaudio)\n",
      "  Using cached triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->torchaudio)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Collecting resampy (from torchvggish)\n",
      "  Using cached resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.5.10)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numba>=0.53 in /opt/conda/lib/python3.10/site-packages (from resampy->torchvggish) (0.59.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.53->resampy->torchvggish) (0.42.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.4.0->torchaudio) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.4.0->torchaudio) (1.3.0)\n",
      "Using cached torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
      "Using cached torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
      "Using cached resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, soundfile, resampy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvggish, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0.post304\n",
      "    Uninstalling torch-2.0.0.post304:\n",
      "      Successfully uninstalled torch-2.0.0.post304\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.4.0 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.4.0 which is incompatible.\n",
      "fastai 2.7.15 requires torch<2.4,>=1.10, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 resampy-0.4.3 soundfile-0.12.1 torch-2.4.0 torchaudio-2.4.0 torchvggish-0.2 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchaudio pandas torchvggish nltk soundfile numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176874f4-cc8a-4371-9186-e1cc4ecfab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import gc\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# 1. Import libraries and set up environment\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] >= self.freq_threshold:\n",
    "                    if word not in self.stoi:\n",
    "                        self.stoi[word] = idx\n",
    "                        self.itos[idx] = word\n",
    "                        idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]\n",
    "# 3. Dataset and DataLoader\n",
    "\n",
    "class MusicCapsDataset(Dataset):\n",
    "    def __init__(self, csv_files, audio_dir, vocab, fixed_length=160000, n_mels=128, target_sample_rate=16000):\n",
    "        # Merge all CSVs into a single DataFrame\n",
    "        dataframes = [pd.read_csv(csv_file) for csv_file in csv_files]\n",
    "        self.data = pd.concat(dataframes, ignore_index=True)\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        print(self.data.shape)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.vocab = vocab\n",
    "        self.fixed_length = fixed_length\n",
    "        self.n_mels = n_mels\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.target_sample_rate,\n",
    "            n_mels=self.n_mels\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_id = self.data.iloc[idx]['ytid']\n",
    "        audio_path = os.path.join(self.audio_dir, f\"{audio_id}.wav\")\n",
    "        caption = self.data.iloc[idx]['caption']\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sample_rate)(waveform)\n",
    "\n",
    "        if waveform.size(1) > self.fixed_length:\n",
    "            waveform = waveform[:, :self.fixed_length]\n",
    "        else:\n",
    "            pad_length = self.fixed_length - waveform.size(1)\n",
    "            waveform = F.pad(waveform, (0, pad_length))\n",
    "\n",
    "        mel_spectrogram = self.mel_spectrogram(waveform)\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]] + self.vocab.numericalize(caption) + [self.vocab.stoi[\"<EOS>\"]]\n",
    "\n",
    "        sample = {\n",
    "            'audio': mel_spectrogram.squeeze(0).T,  # Transpose to match (time, n_mels)\n",
    "            'caption': torch.tensor(numericalized_caption),\n",
    "            'original_caption': caption,\n",
    "            'path': audio_path,\n",
    "            'sample_rate': self.target_sample_rate\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "def pad_sequence(batch):\n",
    "    batch = [item['caption'] for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    return batch\n",
    "\n",
    "def collate_fn(data):\n",
    "    audio = [item['audio'] for item in data]\n",
    "    captions = [item['caption'] for item in data]\n",
    "\n",
    "    # Pad audio sequences to the same length\n",
    "    audio_lengths = [len(a) for a in audio]\n",
    "    max_audio_length = max(audio_lengths)\n",
    "    padded_audio = torch.zeros(len(audio), max_audio_length, audio[0].size(1))\n",
    "    for i, a in enumerate(audio):\n",
    "        padded_audio[i, :len(a), :] = a\n",
    "\n",
    "    # Pad caption sequences to the same length\n",
    "    caption_lengths = [len(c) for c in captions]\n",
    "    max_caption_length = max(caption_lengths)\n",
    "    padded_captions = torch.zeros(len(captions), max_caption_length).long()\n",
    "    for i, c in enumerate(captions):\n",
    "        padded_captions[i, :len(c)] = c\n",
    "\n",
    "    original_captions = [item['original_caption'] for item in data]\n",
    "    paths = [item['path'] for item in data]\n",
    "\n",
    "    return {'audio': padded_audio, 'caption': padded_captions, 'original_caption': original_captions, 'path': paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b9e5e-c1e1-4eaa-ac0d-14aebdbea6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, n_mels, d_model, nhead, num_encoder_layers, dim_feedforward):\n",
    "        super(AudioEncoder, self).__init__()\n",
    "        self.conv = nn.Conv1d(n_mels, d_model, kernel_size=3, padding=1)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward), \n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (batch_size, time_steps, n_mels)\n",
    "        src = self.conv(src.transpose(1, 2))  # (batch_size, d_model, time_steps)\n",
    "        src = src.transpose(1, 2)  # (batch_size, time_steps, d_model)\n",
    "        src = self.positional_encoding(src)  # (batch_size, time_steps, d_model)\n",
    "        output = self.transformer_encoder(src.transpose(0, 1))  # (time_steps, batch_size, d_model)\n",
    "        return output.transpose(0, 1)  # (batch_size, time_steps, d_model)\n",
    "\n",
    "class CaptionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_decoder_layers, dim_feedforward):\n",
    "        super(CaptionDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward),\n",
    "            num_layers=num_decoder_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        # tgt shape: (batch_size, tgt_len)\n",
    "        tgt = self.embedding(tgt)  # (batch_size, tgt_len, d_model)\n",
    "        tgt = self.positional_encoding(tgt.transpose(0, 1))  # (tgt_len, batch_size, d_model)\n",
    "        output = self.transformer_decoder(tgt, memory.transpose(0, 1))  # (tgt_len, batch_size, d_model)\n",
    "        output = self.fc_out(output)  # (tgt_len, batch_size, vocab_size)\n",
    "        return output.transpose(0, 1)  # (batch_size, tgt_len, vocab_size)\n",
    "\n",
    "class AudioCaptioningModel(nn.Module):\n",
    "    def __init__(self, n_mels, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048):\n",
    "        super(AudioCaptioningModel, self).__init__()\n",
    "        self.encoder = AudioEncoder(n_mels, d_model, nhead, num_encoder_layers, dim_feedforward)\n",
    "        self.decoder = CaptionDecoder(vocab_size, d_model, nhead, num_decoder_layers, dim_feedforward)\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "            \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        memory = self.encoder(src)\n",
    "        \n",
    "        # Shift the target to the left (remove the last token for input)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_expected = tgt[:, 1:]  # The expected output, shifted by one\n",
    "    \n",
    "        # Embed and apply positional encoding to the target sequence\n",
    "        tgt_embedded = self.decoder.embedding(tgt_input)\n",
    "        tgt_embedded = self.decoder.positional_encoding(tgt_embedded.transpose(0, 1))  # (tgt_len, batch_size, d_model)\n",
    "    \n",
    "        # Generate a causal mask for the decoder (prevent attending to future tokens)\n",
    "        tgt_len = tgt_input.size(1)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(tgt.device)  # (tgt_len, tgt_len)\n",
    "    \n",
    "        # Generate padding mask (optional if padding is used)\n",
    "        if hasattr(self.decoder.embedding, 'padding_idx') and self.decoder.embedding.padding_idx is not None:\n",
    "            # Create the padding mask as a boolean tensor\n",
    "            tgt_padding_mask = (tgt_input == self.decoder.embedding.padding_idx)  # (batch_size, tgt_len)\n",
    "        else:\n",
    "            tgt_padding_mask = None\n",
    "    \n",
    "        # Apply the transformer decoder\n",
    "        output = self.decoder.transformer_decoder(\n",
    "            tgt_embedded, \n",
    "            memory.transpose(0, 1),\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Apply the final linear layer\n",
    "        output = self.decoder.fc_out(output)  # (tgt_len, batch_size, vocab_size)\n",
    "        return output.transpose(0, 1)  # (batch_size, tgt_len, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee0ad1-d7d5-4430-a196-fb19722dc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoints'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model, epoch_number):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, epoch_number)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, epoch_number)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, epoch_number):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), f'{self.path}/epoch_{epoch_number}_loss_{val_loss}')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, dataloader, criterion, optimizer, epoch, device, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch_idx, data in enumerate(progress_bar):\n",
    "        src = data['audio'].to(device)\n",
    "        tgt = data['caption'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt, teacher_forcing_ratio)\n",
    "        \n",
    "        # Reshape the output and target for computing the loss\n",
    "        output = output.reshape(-1, model.vocab_size)\n",
    "        tgt_expected = tgt[:, 1:].contiguous().view(-1)  # Shifted target for comparison\n",
    "\n",
    "        loss = criterion(output, tgt_expected)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Update tqdm progress bar with the current loss\n",
    "        progress_bar.set_postfix(loss=epoch_loss / (batch_idx + 1))\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# Validation function (unchanged)\n",
    "def validate_epoch(model, dataloader, criterion, epoch, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(tqdm(dataloader, desc=f\"Validating Epoch {epoch+1}\")):\n",
    "            src = data['audio'].to(device)\n",
    "            tgt = data['caption'].to(device)\n",
    "\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0)  # No teacher forcing during validation\n",
    "\n",
    "            # Reshape the output and target for computing the loss\n",
    "            output = output.reshape(-1, model.vocab_size)\n",
    "            tgt_expected = tgt[:, 1:].contiguous().view(-1)  # Shifted target for comparison\n",
    "\n",
    "            loss = criterion(output, tgt_expected)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# Full training loop with early stopping and learning rate scheduler\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=20, teacher_forcing_ratio=0.5, patience=5, model_save_path='checkpoints5', scheduler=None):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=model_save_path)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, epoch, device, teacher_forcing_ratio)\n",
    "        val_loss = validate_epoch(model, val_loader, criterion, epoch, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        print(infer(audio_path='./song_eden.wav', model=model, vocab=vocab, device=device))\n",
    "\n",
    "        # Step the scheduler based on validation loss if provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        early_stopping(val_loss, model, epoch)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07562857-da5b-4b22-8d99-1b207f5a774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def infer(audio_path, model, vocab, fixed_length=160000, n_mels=128, target_sample_rate=16000, max_caption_length=50, device='cuda'):\n",
    "    # Load and preprocess the audio\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)(waveform)\n",
    "\n",
    "    # Pad or truncate waveform to fixed length\n",
    "    if waveform.size(1) > fixed_length:\n",
    "        waveform = waveform[:, :fixed_length]\n",
    "    else:\n",
    "        pad_length = fixed_length - waveform.size(1)\n",
    "        waveform = F.pad(waveform, (0, pad_length))\n",
    "\n",
    "    # Convert waveform to mel spectrogram\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=target_sample_rate,\n",
    "        n_mels=n_mels\n",
    "    )(waveform)\n",
    "\n",
    "    # Prepare the input tensor for the model\n",
    "    mel_spectrogram = mel_spectrogram.squeeze(0).T.unsqueeze(0).to(device)  # (1, time_steps, n_mels)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the input for the decoder (start with the <SOS> token)\n",
    "    tgt_input = torch.tensor([[vocab.stoi['<SOS>']]], device=device)  # Shape: (1, 1)\n",
    "\n",
    "    # Run the encoder\n",
    "    with torch.no_grad():\n",
    "        memory = model.encoder(mel_spectrogram)\n",
    "\n",
    "    # Initialize a list to store generated tokens\n",
    "    generated_tokens = []\n",
    "\n",
    "    # Generate the caption using the decoder\n",
    "    for _ in range(max_caption_length):\n",
    "        # Embed and apply positional encoding to the target input\n",
    "        tgt_embedded = model.decoder.embedding(tgt_input)\n",
    "        tgt_embedded = model.decoder.positional_encoding(tgt_embedded.transpose(0, 1))  # (tgt_len, batch_size, d_model)\n",
    "\n",
    "        # Generate a causal mask for the decoder\n",
    "        tgt_len = tgt_input.size(1)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(device)  # (tgt_len, tgt_len)\n",
    "\n",
    "        # Run the decoder\n",
    "        output = model.decoder.transformer_decoder(\n",
    "            tgt_embedded,\n",
    "            memory.transpose(0, 1),\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=None  # No padding mask needed during inference\n",
    "        )\n",
    "\n",
    "        # Apply the final linear layer to get logits\n",
    "        output = model.decoder.fc_out(output)  # (tgt_len, batch_size, vocab_size)\n",
    "\n",
    "        # Get the predicted token for the current step\n",
    "        next_token = output[-1, :, :].argmax(-1).unsqueeze(0)  # Shape: (1, batch_size)\n",
    "\n",
    "        # Transpose next_token to match tgt_input shape for concatenation\n",
    "        next_token = next_token.transpose(0, 1)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Append the predicted token to the input sequence and to the generated tokens\n",
    "        tgt_input = torch.cat([tgt_input, next_token], dim=1)\n",
    "        generated_tokens.append(next_token.item())\n",
    "\n",
    "        # Stop if <EOS> token is generated\n",
    "        if next_token.item() == vocab.stoi['<EOS>']:\n",
    "            break\n",
    "\n",
    "    # Convert the generated sequence of tokens to words\n",
    "    generated_caption = [vocab.itos[token] for token in generated_tokens]\n",
    "\n",
    "    return ' '.join(generated_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c214738d-8277-45ea-ac23-863ebc201043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6312, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and prepare data\n",
    "csv_files = ['mayo_final_final.csv', 'musiccaps-public.csv']\n",
    "# csv_files = ['musiccaps-public.csv']\n",
    "audio_dir = './music_data/music_data'\n",
    "freq_threshold = 5\n",
    "\n",
    "dataframes = [pd.read_csv(csv_file) for csv_file in csv_files]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Split dataset\n",
    "dataset = MusicCapsDataset(csv_files=csv_files, audio_dir=audio_dir, vocab=None)  # Pass vocab=None for now\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = math.trunc((len(dataset) - train_size) / 2)\n",
    "random_seed = 42\n",
    "generator = torch.Generator().manual_seed(random_seed)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size + 1, val_size], generator)\n",
    "\n",
    "# Build vocabulary from training set\n",
    "\n",
    "train_captions = [df.iloc[idx]['caption'] for idx in train_dataset.indices]\n",
    "vocab = Vocabulary(freq_threshold)\n",
    "vocab.build_vocabulary(train_captions)\n",
    "\n",
    "# Update datasets with vocab\n",
    "train_dataset.dataset.vocab = vocab\n",
    "val_dataset.dataset.vocab = vocab\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Instantiate the model\n",
    "model = AudioCaptioningModel(\n",
    "    n_mels=128, \n",
    "    vocab_size=len(vocab), \n",
    "    d_model=512, \n",
    "    nhead=8, \n",
    "    num_encoder_layers=6, \n",
    "    num_decoder_layers=6, \n",
    "    dim_feedforward=2048\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0002)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# scheduler = None\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "# # Optimizer and scheduler\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef5f7c-a190-4e9c-a3a6-4b3c62f0aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vocab, 'vocab4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ef18b-70b4-4846-8675-e04f356ac1dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|          | 1/158 [00:02<05:24,  2.07s/it, loss=7.75]"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler=scheduler, num_epochs=num_epochs, device=device, teacher_forcing_ratio=0.1)\n",
    "# train_model(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler=NoneDâ€ž, num_epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d598093-1d9e-48ee-8109-baee1147090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_loss(eval_losses, train_losses, number_of_epochs):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, number_of_epochs + 1), eval_losses, marker='o', linestyle='-', color='b', label='val')\n",
    "    plt.plot(range(1, number_of_epochs + 1), train_losses, marker='o', linestyle='-', color='r', label='train')\n",
    "    plt.title('Validation Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper left', fontsize='large', frameon=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "visualize_loss(val_losses, train_losses, number_of_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadcc4ca-bd83-457b-8b1f-d6a3804e6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_epoch(model, val_dataloader, criterion, 1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4316c-665d-4626-a379-a31c45747d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open(\"./new-dill.pkl\", 'wb') as file:\n",
    "    dill.dump(vocab, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f87293b-45be-4bb6-908e-f3bd5e4d967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(audio_path='./song_eden.wav', model=model, vocab=vocab, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03841e0-67e0-404b-9974-0aaec766f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(audio_path='./mozart.wav', model=model, vocab=vocab, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0497d-ecf4-4b0c-b1ff-c3962f3012e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(audio_path='./kendrick.wav', model=model, vocab=vocab, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42aabd9-2572-4334-93a7-103877d71779",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c175817-17b1-4865-bbde-3139b36f52e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(audio_path='./music_data/music_data/4yJZ4VX8XQI.wav', model=model, vocab=vocab, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
